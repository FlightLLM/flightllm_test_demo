- name: round_1_input_layer
  type: input
  input: []
  output:
  - name: round_1_hidden_states_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_input_layer_norm_layer_0
  type: layernorm
  input:
  - name: round_1_hidden_states_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_0.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_0.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_0.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_0
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_0
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_0
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_0.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_0.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_0
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_0
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_0
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_0.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_0.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_0
  type: concat
  input:
  - name: round_0_key_t_0
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_0
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_0
  type: concat
  input:
  - name: round_1_past_values_0
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_0
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_0
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_0
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_0
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_0.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_0
  type: softmax
  input:
  - name: round_1_qkt_mm_0
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_0
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_0
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_0
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_0
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_0
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_0.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_0
  type: linear_mv
  input:
  - name: round_1_qktv_t_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_0.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_0.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_0
  type: eltwise
  input:
  - name: round_1_hidden_states_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_0
  type: layernorm
  input:
  - name: round_1_hidden_sum_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_0.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_0.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_0
  type: silu
  input:
  - name: round_1_hidden_gate_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_0.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_0
  type: eltwise
  input:
  - name: round_1_hidden_up_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_0
  type: linear_mv
  input:
  - name: round_1_hidden_mul_0
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_0.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_0
  type: eltwise
  input:
  - name: round_1_hidden_sum_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_0
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_1
  type: layernorm
  input:
  - name: round_1_hidden_states_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_1.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_1.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_1.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_1
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_1
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_1
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_1.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_1.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_1
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_1
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_1
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_1.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_1.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_1
  type: concat
  input:
  - name: round_0_key_t_1
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_1
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_1
  type: concat
  input:
  - name: round_1_past_values_1
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_1
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_1
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_1
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_1
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_1.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_1
  type: softmax
  input:
  - name: round_1_qkt_mm_1
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_1
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_1
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_1
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_1
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_1
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_1.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_1
  type: linear_mv
  input:
  - name: round_1_qktv_t_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_1.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_1.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_1
  type: eltwise
  input:
  - name: round_1_hidden_states_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_1
  type: layernorm
  input:
  - name: round_1_hidden_sum_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_1.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_1.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_1
  type: silu
  input:
  - name: round_1_hidden_gate_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_1.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_1
  type: eltwise
  input:
  - name: round_1_hidden_up_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_1
  type: linear_mv
  input:
  - name: round_1_hidden_mul_1
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_1.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_1
  type: eltwise
  input:
  - name: round_1_hidden_sum_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_1
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_2
  type: layernorm
  input:
  - name: round_1_hidden_states_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_2.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_2.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_2.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_2
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_2
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_2
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_2.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_2.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_2
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_2
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_2
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_2.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_2.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_2
  type: concat
  input:
  - name: round_0_key_t_2
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_2
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_2
  type: concat
  input:
  - name: round_1_past_values_2
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_2
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_2
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_2
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_2
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_2.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_2
  type: softmax
  input:
  - name: round_1_qkt_mm_2
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_2
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_2
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_2
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_2
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_2
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_2.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_2
  type: linear_mv
  input:
  - name: round_1_qktv_t_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_2.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_2.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_2
  type: eltwise
  input:
  - name: round_1_hidden_states_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_2
  type: layernorm
  input:
  - name: round_1_hidden_sum_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_2.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_2.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_2
  type: silu
  input:
  - name: round_1_hidden_gate_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_2.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_2
  type: eltwise
  input:
  - name: round_1_hidden_up_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_2
  type: linear_mv
  input:
  - name: round_1_hidden_mul_2
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_2.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_2
  type: eltwise
  input:
  - name: round_1_hidden_sum_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_2
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_3
  type: layernorm
  input:
  - name: round_1_hidden_states_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_3.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_3.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_3.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_3
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_3
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_3
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_3.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_3.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_3
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_3
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_3
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_3.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_3.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_3
  type: concat
  input:
  - name: round_0_key_t_3
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_3
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_3
  type: concat
  input:
  - name: round_1_past_values_3
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_3
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_3
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_3
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_3
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_3.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_3
  type: softmax
  input:
  - name: round_1_qkt_mm_3
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_3
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_3
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_3
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_3
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_3
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_3.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_3
  type: linear_mv
  input:
  - name: round_1_qktv_t_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_3.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_3.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_3
  type: eltwise
  input:
  - name: round_1_hidden_states_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_3
  type: layernorm
  input:
  - name: round_1_hidden_sum_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_3.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_3.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_3
  type: silu
  input:
  - name: round_1_hidden_gate_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_3.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_3
  type: eltwise
  input:
  - name: round_1_hidden_up_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_3
  type: linear_mv
  input:
  - name: round_1_hidden_mul_3
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_3.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_3
  type: eltwise
  input:
  - name: round_1_hidden_sum_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_3
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_4
  type: layernorm
  input:
  - name: round_1_hidden_states_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_4.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_4.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_4.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_4
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_4
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_4
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_4.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_4.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_4
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_4
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_4
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_4.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_4.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_4
  type: concat
  input:
  - name: round_0_key_t_4
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_4
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_4
  type: concat
  input:
  - name: round_1_past_values_4
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_4
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_4
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_4
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_4
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_4.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_4
  type: softmax
  input:
  - name: round_1_qkt_mm_4
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_4
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_4
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_4
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_4
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_4
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_4.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_4
  type: linear_mv
  input:
  - name: round_1_qktv_t_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_4.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_4.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_4
  type: eltwise
  input:
  - name: round_1_hidden_states_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_4
  type: layernorm
  input:
  - name: round_1_hidden_sum_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_4.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_4.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_4
  type: silu
  input:
  - name: round_1_hidden_gate_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_4.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_4
  type: eltwise
  input:
  - name: round_1_hidden_up_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_4
  type: linear_mv
  input:
  - name: round_1_hidden_mul_4
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_4.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_4
  type: eltwise
  input:
  - name: round_1_hidden_sum_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_4
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_5
  type: layernorm
  input:
  - name: round_1_hidden_states_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_5.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_5.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_5.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_5
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_5
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_5
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_5.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_5.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_5
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_5
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_5
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_5.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_5.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_5
  type: concat
  input:
  - name: round_0_key_t_5
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_5
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_5
  type: concat
  input:
  - name: round_1_past_values_5
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_5
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_5
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_5
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_5
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_5.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_5
  type: softmax
  input:
  - name: round_1_qkt_mm_5
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_5
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_5
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_5
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_5
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_5
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_5.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_5
  type: linear_mv
  input:
  - name: round_1_qktv_t_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_5.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_5.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_5
  type: eltwise
  input:
  - name: round_1_hidden_states_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_5
  type: layernorm
  input:
  - name: round_1_hidden_sum_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_5.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_5.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_5
  type: silu
  input:
  - name: round_1_hidden_gate_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_5.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_5
  type: eltwise
  input:
  - name: round_1_hidden_up_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_5
  type: linear_mv
  input:
  - name: round_1_hidden_mul_5
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_5.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_5
  type: eltwise
  input:
  - name: round_1_hidden_sum_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_5
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_6
  type: layernorm
  input:
  - name: round_1_hidden_states_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_6.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_6.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_6.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_6
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_6
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_6
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_6.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_6.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_6
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_6
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_6
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_6.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_6.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_6
  type: concat
  input:
  - name: round_0_key_t_6
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_6
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_6
  type: concat
  input:
  - name: round_1_past_values_6
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_6
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_6
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_6
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_6
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_6.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_6
  type: softmax
  input:
  - name: round_1_qkt_mm_6
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_6
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_6
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_6
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_6
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_6
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_6.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_6
  type: linear_mv
  input:
  - name: round_1_qktv_t_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_6.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_6.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_6
  type: eltwise
  input:
  - name: round_1_hidden_states_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_6
  type: layernorm
  input:
  - name: round_1_hidden_sum_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_6.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_6.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_6
  type: silu
  input:
  - name: round_1_hidden_gate_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_6.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_6
  type: eltwise
  input:
  - name: round_1_hidden_up_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_6
  type: linear_mv
  input:
  - name: round_1_hidden_mul_6
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_6.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_6
  type: eltwise
  input:
  - name: round_1_hidden_sum_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_6
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_7
  type: layernorm
  input:
  - name: round_1_hidden_states_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_7.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_7.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_7.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_7
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_7
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_7
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_7.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_7.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_7
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_7
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_7
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_7.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_7.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_7
  type: concat
  input:
  - name: round_0_key_t_7
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_7
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_7
  type: concat
  input:
  - name: round_1_past_values_7
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_7
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_7
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_7
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_7
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_7.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_7
  type: softmax
  input:
  - name: round_1_qkt_mm_7
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_7
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_7
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_7
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_7
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_7
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_7.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_7
  type: linear_mv
  input:
  - name: round_1_qktv_t_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_7.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_7.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_7
  type: eltwise
  input:
  - name: round_1_hidden_states_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_7
  type: layernorm
  input:
  - name: round_1_hidden_sum_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_7.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_7.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_7
  type: silu
  input:
  - name: round_1_hidden_gate_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_7.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_7
  type: eltwise
  input:
  - name: round_1_hidden_up_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_7
  type: linear_mv
  input:
  - name: round_1_hidden_mul_7
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_7.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_7
  type: eltwise
  input:
  - name: round_1_hidden_sum_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_7
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_8
  type: layernorm
  input:
  - name: round_1_hidden_states_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_8.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_8.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_8.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_8
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_8
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_8
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_8.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_8.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_8
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_8
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_8
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_8.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_8.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_8
  type: concat
  input:
  - name: round_0_key_t_8
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_8
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_8
  type: concat
  input:
  - name: round_1_past_values_8
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_8
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_8
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_8
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_8
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_8.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_8
  type: softmax
  input:
  - name: round_1_qkt_mm_8
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_8
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_8
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_8
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_8
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_8
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_8.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_8
  type: linear_mv
  input:
  - name: round_1_qktv_t_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_8.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_8.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_8
  type: eltwise
  input:
  - name: round_1_hidden_states_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_8
  type: layernorm
  input:
  - name: round_1_hidden_sum_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_8.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_8.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_8
  type: silu
  input:
  - name: round_1_hidden_gate_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_8.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_8
  type: eltwise
  input:
  - name: round_1_hidden_up_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_8
  type: linear_mv
  input:
  - name: round_1_hidden_mul_8
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_8.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_8
  type: eltwise
  input:
  - name: round_1_hidden_sum_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_8
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_9
  type: layernorm
  input:
  - name: round_1_hidden_states_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_9.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_9.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_9.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_9
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_9
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_9
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_9.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_9.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_9
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_9
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_9
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_9.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_9.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_9
  type: concat
  input:
  - name: round_0_key_t_9
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_9
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_9
  type: concat
  input:
  - name: round_1_past_values_9
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_9
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_9
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_9
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_9
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_9.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_9
  type: softmax
  input:
  - name: round_1_qkt_mm_9
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_9
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_9
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_9
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_9
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_9
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_9.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_9
  type: linear_mv
  input:
  - name: round_1_qktv_t_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_9.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_9.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_9
  type: eltwise
  input:
  - name: round_1_hidden_states_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_9
  type: layernorm
  input:
  - name: round_1_hidden_sum_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_9.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_9.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_9
  type: silu
  input:
  - name: round_1_hidden_gate_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_9.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_9
  type: eltwise
  input:
  - name: round_1_hidden_up_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_9
  type: linear_mv
  input:
  - name: round_1_hidden_mul_9
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_9.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_9
  type: eltwise
  input:
  - name: round_1_hidden_sum_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_9
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_10
  type: layernorm
  input:
  - name: round_1_hidden_states_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_10.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_10.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_10.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_10
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_10
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_10
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_10.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_10.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_10
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_10
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_10
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_10.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_10.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_10
  type: concat
  input:
  - name: round_0_key_t_10
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_10
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_10
  type: concat
  input:
  - name: round_1_past_values_10
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_10
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_10
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_10
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_10
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_10.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_10
  type: softmax
  input:
  - name: round_1_qkt_mm_10
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_10
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_10
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_10
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_10
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_10
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_10.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_10
  type: linear_mv
  input:
  - name: round_1_qktv_t_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_10.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_10.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_10
  type: eltwise
  input:
  - name: round_1_hidden_states_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_10
  type: layernorm
  input:
  - name: round_1_hidden_sum_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_10.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_10.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_10
  type: silu
  input:
  - name: round_1_hidden_gate_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_10.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_10
  type: eltwise
  input:
  - name: round_1_hidden_up_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_10
  type: linear_mv
  input:
  - name: round_1_hidden_mul_10
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_10.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_10
  type: eltwise
  input:
  - name: round_1_hidden_sum_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_10
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_11
  type: layernorm
  input:
  - name: round_1_hidden_states_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_11.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_11.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_11.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_11
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_11
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_11
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_11.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_11.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_11
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_11
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_11
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_11.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_11.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_11
  type: concat
  input:
  - name: round_0_key_t_11
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_11
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_11
  type: concat
  input:
  - name: round_1_past_values_11
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_11
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_11
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_11
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_11
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_11.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_11
  type: softmax
  input:
  - name: round_1_qkt_mm_11
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_11
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_11
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_11
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_11
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_11
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_11.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_11
  type: linear_mv
  input:
  - name: round_1_qktv_t_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_11.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_11.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_11
  type: eltwise
  input:
  - name: round_1_hidden_states_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_11
  type: layernorm
  input:
  - name: round_1_hidden_sum_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_11.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_11.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_11
  type: silu
  input:
  - name: round_1_hidden_gate_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_11.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_11
  type: eltwise
  input:
  - name: round_1_hidden_up_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_11
  type: linear_mv
  input:
  - name: round_1_hidden_mul_11
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_11.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_11
  type: eltwise
  input:
  - name: round_1_hidden_sum_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_11
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_12
  type: layernorm
  input:
  - name: round_1_hidden_states_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_12.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_12.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_12.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_12
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_12
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_12
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_12.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_12.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_12
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_12
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_12
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_12.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_12.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_12
  type: concat
  input:
  - name: round_0_key_t_12
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_12
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_12
  type: concat
  input:
  - name: round_1_past_values_12
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_12
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_12
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_12
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_12
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_12.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_12
  type: softmax
  input:
  - name: round_1_qkt_mm_12
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_12
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_12
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_12
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_12
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_12
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_12.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_12
  type: linear_mv
  input:
  - name: round_1_qktv_t_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_12.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_12.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_12
  type: eltwise
  input:
  - name: round_1_hidden_states_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_12
  type: layernorm
  input:
  - name: round_1_hidden_sum_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_12.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_12.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_12
  type: silu
  input:
  - name: round_1_hidden_gate_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_12.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_12
  type: eltwise
  input:
  - name: round_1_hidden_up_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_12
  type: linear_mv
  input:
  - name: round_1_hidden_mul_12
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_12.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_12
  type: eltwise
  input:
  - name: round_1_hidden_sum_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_12
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_13
  type: layernorm
  input:
  - name: round_1_hidden_states_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_13.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_13.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_13.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_13
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_13
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_13
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_13.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_13.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_13
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_13
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_13
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_13.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_13.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_13
  type: concat
  input:
  - name: round_0_key_t_13
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_13
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_13
  type: concat
  input:
  - name: round_1_past_values_13
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_13
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_13
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_13
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_13
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_13.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_13
  type: softmax
  input:
  - name: round_1_qkt_mm_13
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_13
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_13
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_13
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_13
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_13
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_13.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_13
  type: linear_mv
  input:
  - name: round_1_qktv_t_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_13.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_13.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_13
  type: eltwise
  input:
  - name: round_1_hidden_states_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_13
  type: layernorm
  input:
  - name: round_1_hidden_sum_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_13.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_13.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_13
  type: silu
  input:
  - name: round_1_hidden_gate_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_13.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_13
  type: eltwise
  input:
  - name: round_1_hidden_up_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_13
  type: linear_mv
  input:
  - name: round_1_hidden_mul_13
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_13.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_13
  type: eltwise
  input:
  - name: round_1_hidden_sum_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_13
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_14
  type: layernorm
  input:
  - name: round_1_hidden_states_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_14.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_14.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_14.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_14
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_14
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_14
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_14.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_14.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_14
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_14
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_14
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_14.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_14.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_14
  type: concat
  input:
  - name: round_0_key_t_14
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_14
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_14
  type: concat
  input:
  - name: round_1_past_values_14
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_14
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_14
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_14
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_14
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_14.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_14
  type: softmax
  input:
  - name: round_1_qkt_mm_14
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_14
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_14
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_14
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_14
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_14
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_14.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_14
  type: linear_mv
  input:
  - name: round_1_qktv_t_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_14.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_14.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_14
  type: eltwise
  input:
  - name: round_1_hidden_states_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_14
  type: layernorm
  input:
  - name: round_1_hidden_sum_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_14.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_14.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_14
  type: silu
  input:
  - name: round_1_hidden_gate_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_14.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_14
  type: eltwise
  input:
  - name: round_1_hidden_up_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_14
  type: linear_mv
  input:
  - name: round_1_hidden_mul_14
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_14.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_14
  type: eltwise
  input:
  - name: round_1_hidden_sum_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_14
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_15
  type: layernorm
  input:
  - name: round_1_hidden_states_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_15.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_15.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_15.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_15
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_15
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_15
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_15.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_15.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_15
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_15
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_15
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_15.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_15.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_15
  type: concat
  input:
  - name: round_0_key_t_15
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_15
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_15
  type: concat
  input:
  - name: round_1_past_values_15
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_15
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_15
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_15
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_15
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_15.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_15
  type: softmax
  input:
  - name: round_1_qkt_mm_15
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_15
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_15
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_15
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_15
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_15
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_15.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_15
  type: linear_mv
  input:
  - name: round_1_qktv_t_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_15.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_15.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_15
  type: eltwise
  input:
  - name: round_1_hidden_states_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_15
  type: layernorm
  input:
  - name: round_1_hidden_sum_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_15.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_15.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_15
  type: silu
  input:
  - name: round_1_hidden_gate_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_15.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_15
  type: eltwise
  input:
  - name: round_1_hidden_up_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_15
  type: linear_mv
  input:
  - name: round_1_hidden_mul_15
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_15.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_15
  type: eltwise
  input:
  - name: round_1_hidden_sum_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_15
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_16
  type: layernorm
  input:
  - name: round_1_hidden_states_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_16.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_16.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_16.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_16
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_16
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_16
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_16.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_16.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_16
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_16
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_16
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_16.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_16.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_16
  type: concat
  input:
  - name: round_0_key_t_16
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_16
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_16
  type: concat
  input:
  - name: round_1_past_values_16
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_16
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_16
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_16
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_16
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_16.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_16
  type: softmax
  input:
  - name: round_1_qkt_mm_16
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_16
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_16
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_16
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_16
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_16
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_16.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_16
  type: linear_mv
  input:
  - name: round_1_qktv_t_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_16.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_16.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_16
  type: eltwise
  input:
  - name: round_1_hidden_states_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_16
  type: layernorm
  input:
  - name: round_1_hidden_sum_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_16.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_16.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_16
  type: silu
  input:
  - name: round_1_hidden_gate_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_16.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_16
  type: eltwise
  input:
  - name: round_1_hidden_up_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_16
  type: linear_mv
  input:
  - name: round_1_hidden_mul_16
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_16.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_16
  type: eltwise
  input:
  - name: round_1_hidden_sum_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_16
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_17
  type: layernorm
  input:
  - name: round_1_hidden_states_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_17.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_17.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_17.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_17
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_17
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_17
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_17.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_17.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_17
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_17
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_17
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_17.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_17.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_17
  type: concat
  input:
  - name: round_0_key_t_17
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_17
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_17
  type: concat
  input:
  - name: round_1_past_values_17
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_17
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_17
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_17
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_17
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_17.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_17
  type: softmax
  input:
  - name: round_1_qkt_mm_17
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_17
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_17
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_17
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_17
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_17
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_17.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_17
  type: linear_mv
  input:
  - name: round_1_qktv_t_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_17.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_17.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_17
  type: eltwise
  input:
  - name: round_1_hidden_states_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_17
  type: layernorm
  input:
  - name: round_1_hidden_sum_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_17.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_17.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_17
  type: silu
  input:
  - name: round_1_hidden_gate_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_17.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_17
  type: eltwise
  input:
  - name: round_1_hidden_up_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_17
  type: linear_mv
  input:
  - name: round_1_hidden_mul_17
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_17.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_17
  type: eltwise
  input:
  - name: round_1_hidden_sum_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_17
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_18
  type: layernorm
  input:
  - name: round_1_hidden_states_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_18.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_18.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_18.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_18
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_18
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_18
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_18.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_18.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_18
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_18
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_18
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_18.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_18.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_18
  type: concat
  input:
  - name: round_0_key_t_18
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_18
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_18
  type: concat
  input:
  - name: round_1_past_values_18
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_18
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_18
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_18
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_18
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_18.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_18
  type: softmax
  input:
  - name: round_1_qkt_mm_18
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_18
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_18
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_18
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_18
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_18
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_18.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_18
  type: linear_mv
  input:
  - name: round_1_qktv_t_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_18.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_18.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_18
  type: eltwise
  input:
  - name: round_1_hidden_states_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_18
  type: layernorm
  input:
  - name: round_1_hidden_sum_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_18.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_18.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_18
  type: silu
  input:
  - name: round_1_hidden_gate_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_18.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_18
  type: eltwise
  input:
  - name: round_1_hidden_up_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_18
  type: linear_mv
  input:
  - name: round_1_hidden_mul_18
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_18.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_18
  type: eltwise
  input:
  - name: round_1_hidden_sum_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_18
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_19
  type: layernorm
  input:
  - name: round_1_hidden_states_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_19.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_19.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_19.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_19
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_19
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_19
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_19.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_19.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_19
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_19
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_19
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_19.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_19.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_19
  type: concat
  input:
  - name: round_0_key_t_19
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_19
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_19
  type: concat
  input:
  - name: round_1_past_values_19
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_19
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_19
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_19
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_19
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_19.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_19
  type: softmax
  input:
  - name: round_1_qkt_mm_19
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_19
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_19
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_19
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_19
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_19
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_19.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_19
  type: linear_mv
  input:
  - name: round_1_qktv_t_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_19.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_19.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_19
  type: eltwise
  input:
  - name: round_1_hidden_states_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_19
  type: layernorm
  input:
  - name: round_1_hidden_sum_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_19.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_19.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_19
  type: silu
  input:
  - name: round_1_hidden_gate_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_19.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_19
  type: eltwise
  input:
  - name: round_1_hidden_up_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_19
  type: linear_mv
  input:
  - name: round_1_hidden_mul_19
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_19.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_19
  type: eltwise
  input:
  - name: round_1_hidden_sum_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_19
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_20
  type: layernorm
  input:
  - name: round_1_hidden_states_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_20.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_20.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_20.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_20
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_20
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_20
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_20.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_20.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_20
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_20
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_20
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_20.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_20.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_20
  type: concat
  input:
  - name: round_0_key_t_20
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_20
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_20
  type: concat
  input:
  - name: round_1_past_values_20
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_20
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_20
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_20
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_20
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_20.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_20
  type: softmax
  input:
  - name: round_1_qkt_mm_20
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_20
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_20
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_20
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_20
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_20
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_20.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_20
  type: linear_mv
  input:
  - name: round_1_qktv_t_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_20.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_20.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_20
  type: eltwise
  input:
  - name: round_1_hidden_states_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_20
  type: layernorm
  input:
  - name: round_1_hidden_sum_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_20.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_20.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_20
  type: silu
  input:
  - name: round_1_hidden_gate_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_20.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_20
  type: eltwise
  input:
  - name: round_1_hidden_up_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_20
  type: linear_mv
  input:
  - name: round_1_hidden_mul_20
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_20.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_20
  type: eltwise
  input:
  - name: round_1_hidden_sum_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_20
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_21
  type: layernorm
  input:
  - name: round_1_hidden_states_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_21.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_21.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_21.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_21
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_21
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_21
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_21.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_21.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_21
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_21
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_21
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_21.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_21.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_21
  type: concat
  input:
  - name: round_0_key_t_21
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_21
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_21
  type: concat
  input:
  - name: round_1_past_values_21
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_21
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_21
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_21
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_21
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_21.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_21
  type: softmax
  input:
  - name: round_1_qkt_mm_21
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_21
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_21
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_21
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_21
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_21
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_21.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_21
  type: linear_mv
  input:
  - name: round_1_qktv_t_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_21.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_21.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_21
  type: eltwise
  input:
  - name: round_1_hidden_states_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_21
  type: layernorm
  input:
  - name: round_1_hidden_sum_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_21.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_21.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_21
  type: silu
  input:
  - name: round_1_hidden_gate_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_21.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_21
  type: eltwise
  input:
  - name: round_1_hidden_up_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_21
  type: linear_mv
  input:
  - name: round_1_hidden_mul_21
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_21.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_21
  type: eltwise
  input:
  - name: round_1_hidden_sum_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_21
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_22
  type: layernorm
  input:
  - name: round_1_hidden_states_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_22.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_22.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_22.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_22
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_22
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_22
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_22.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_22.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_22
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_22
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_22
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_22.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_22.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_22
  type: concat
  input:
  - name: round_0_key_t_22
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_22
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_22
  type: concat
  input:
  - name: round_1_past_values_22
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_22
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_22
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_22
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_22
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_22.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_22
  type: softmax
  input:
  - name: round_1_qkt_mm_22
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_22
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_22
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_22
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_22
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_22
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_22.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_22
  type: linear_mv
  input:
  - name: round_1_qktv_t_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_22.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_22.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_22
  type: eltwise
  input:
  - name: round_1_hidden_states_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_22
  type: layernorm
  input:
  - name: round_1_hidden_sum_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_22.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_22.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_22
  type: silu
  input:
  - name: round_1_hidden_gate_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_22.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_22
  type: eltwise
  input:
  - name: round_1_hidden_up_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_22
  type: linear_mv
  input:
  - name: round_1_hidden_mul_22
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_22.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_22
  type: eltwise
  input:
  - name: round_1_hidden_sum_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_22
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_23
  type: layernorm
  input:
  - name: round_1_hidden_states_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_23.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_23.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_23.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_23
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_23
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_23
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_23.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_23.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_23
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_23
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_23
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_23.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_23.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_23
  type: concat
  input:
  - name: round_0_key_t_23
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_23
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_23
  type: concat
  input:
  - name: round_1_past_values_23
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_23
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_23
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_23
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_23
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_23.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_23
  type: softmax
  input:
  - name: round_1_qkt_mm_23
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_23
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_23
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_23
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_23
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_23
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_23.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_23
  type: linear_mv
  input:
  - name: round_1_qktv_t_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_23.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_23.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_23
  type: eltwise
  input:
  - name: round_1_hidden_states_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_23
  type: layernorm
  input:
  - name: round_1_hidden_sum_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_23.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_23.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_23
  type: silu
  input:
  - name: round_1_hidden_gate_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_23.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_23
  type: eltwise
  input:
  - name: round_1_hidden_up_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_23
  type: linear_mv
  input:
  - name: round_1_hidden_mul_23
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_23.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_23
  type: eltwise
  input:
  - name: round_1_hidden_sum_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_23
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_24
  type: layernorm
  input:
  - name: round_1_hidden_states_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_24.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_24.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_24.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_24
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_24
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_24
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_24.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_24.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_24
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_24
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_24
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_24.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_24.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_24
  type: concat
  input:
  - name: round_0_key_t_24
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_24
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_24
  type: concat
  input:
  - name: round_1_past_values_24
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_24
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_24
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_24
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_24
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_24.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_24
  type: softmax
  input:
  - name: round_1_qkt_mm_24
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_24
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_24
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_24
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_24
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_24
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_24.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_24
  type: linear_mv
  input:
  - name: round_1_qktv_t_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_24.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_24.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_24
  type: eltwise
  input:
  - name: round_1_hidden_states_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_24
  type: layernorm
  input:
  - name: round_1_hidden_sum_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_24.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_24.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_24
  type: silu
  input:
  - name: round_1_hidden_gate_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_24.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_24
  type: eltwise
  input:
  - name: round_1_hidden_up_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_24
  type: linear_mv
  input:
  - name: round_1_hidden_mul_24
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_24.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_24
  type: eltwise
  input:
  - name: round_1_hidden_sum_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_24
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_25
  type: layernorm
  input:
  - name: round_1_hidden_states_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_25.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_25.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_25.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_25
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_25
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_25
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_25.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_25.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_25
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_25
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_25
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_25.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_25.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_25
  type: concat
  input:
  - name: round_0_key_t_25
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_25
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_25
  type: concat
  input:
  - name: round_1_past_values_25
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_25
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_25
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_25
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_25
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_25.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_25
  type: softmax
  input:
  - name: round_1_qkt_mm_25
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_25
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_25
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_25
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_25
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_25
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_25.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_25
  type: linear_mv
  input:
  - name: round_1_qktv_t_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_25.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_25.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_25
  type: eltwise
  input:
  - name: round_1_hidden_states_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_25
  type: layernorm
  input:
  - name: round_1_hidden_sum_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_25.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_25.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_25
  type: silu
  input:
  - name: round_1_hidden_gate_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_25.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_25
  type: eltwise
  input:
  - name: round_1_hidden_up_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_25
  type: linear_mv
  input:
  - name: round_1_hidden_mul_25
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_25.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_25
  type: eltwise
  input:
  - name: round_1_hidden_sum_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_25
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_26
  type: layernorm
  input:
  - name: round_1_hidden_states_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_26.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_26.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_26.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_26
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_26
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_26
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_26.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_26.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_26
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_26
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_26
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_26.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_26.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_26
  type: concat
  input:
  - name: round_0_key_t_26
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_26
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_26
  type: concat
  input:
  - name: round_1_past_values_26
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_26
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_26
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_26
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_26
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_26.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_26
  type: softmax
  input:
  - name: round_1_qkt_mm_26
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_26
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_26
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_26
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_26
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_26
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_26.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_26
  type: linear_mv
  input:
  - name: round_1_qktv_t_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_26.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_26.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_26
  type: eltwise
  input:
  - name: round_1_hidden_states_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_26
  type: layernorm
  input:
  - name: round_1_hidden_sum_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_26.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_26.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_26
  type: silu
  input:
  - name: round_1_hidden_gate_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_26.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_26
  type: eltwise
  input:
  - name: round_1_hidden_up_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_26
  type: linear_mv
  input:
  - name: round_1_hidden_mul_26
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_26.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_26
  type: eltwise
  input:
  - name: round_1_hidden_sum_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_26
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_27
  type: layernorm
  input:
  - name: round_1_hidden_states_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_27.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_27.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_27.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_27
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_27
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_27
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_27.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_27.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_27
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_27
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_27
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_27.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_27.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_27
  type: concat
  input:
  - name: round_0_key_t_27
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_27
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_27
  type: concat
  input:
  - name: round_1_past_values_27
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_27
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_27
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_27
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_27
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_27.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_27
  type: softmax
  input:
  - name: round_1_qkt_mm_27
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_27
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_27
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_27
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_27
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_27
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_27.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_27
  type: linear_mv
  input:
  - name: round_1_qktv_t_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_27.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_27.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_27
  type: eltwise
  input:
  - name: round_1_hidden_states_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_27
  type: layernorm
  input:
  - name: round_1_hidden_sum_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_27.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_27.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_27
  type: silu
  input:
  - name: round_1_hidden_gate_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_27.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_27
  type: eltwise
  input:
  - name: round_1_hidden_up_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_27
  type: linear_mv
  input:
  - name: round_1_hidden_mul_27
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_27.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_27
  type: eltwise
  input:
  - name: round_1_hidden_sum_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_27
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_28
  type: layernorm
  input:
  - name: round_1_hidden_states_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_28.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_28.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_28.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_28
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_28
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_28
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_28.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_28.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_28
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_28
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_28
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_28.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_28.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_28
  type: concat
  input:
  - name: round_0_key_t_28
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_28
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_28
  type: concat
  input:
  - name: round_1_past_values_28
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_28
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_28
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_28
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_28
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_28.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_28
  type: softmax
  input:
  - name: round_1_qkt_mm_28
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_28
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_28
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_28
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_28
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_28
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_28.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_28
  type: linear_mv
  input:
  - name: round_1_qktv_t_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_28.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_28.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_28
  type: eltwise
  input:
  - name: round_1_hidden_states_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_28
  type: layernorm
  input:
  - name: round_1_hidden_sum_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_28.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_28.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_28
  type: silu
  input:
  - name: round_1_hidden_gate_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_28.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_28
  type: eltwise
  input:
  - name: round_1_hidden_up_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_28
  type: linear_mv
  input:
  - name: round_1_hidden_mul_28
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_28.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_28
  type: eltwise
  input:
  - name: round_1_hidden_sum_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_28
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_29
  type: layernorm
  input:
  - name: round_1_hidden_states_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_29.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_29.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_29.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_29
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_29
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_29
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_29.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_29.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_29
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_29
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_29
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_29.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_29.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_29
  type: concat
  input:
  - name: round_0_key_t_29
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_29
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_29
  type: concat
  input:
  - name: round_1_past_values_29
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_29
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_29
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_29
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_29
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_29.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_29
  type: softmax
  input:
  - name: round_1_qkt_mm_29
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_29
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_29
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_29
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_29
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_29
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_29.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_29
  type: linear_mv
  input:
  - name: round_1_qktv_t_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_29.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_29.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_29
  type: eltwise
  input:
  - name: round_1_hidden_states_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_29
  type: layernorm
  input:
  - name: round_1_hidden_sum_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_29.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_29.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_29
  type: silu
  input:
  - name: round_1_hidden_gate_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_29.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_29
  type: eltwise
  input:
  - name: round_1_hidden_up_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_29
  type: linear_mv
  input:
  - name: round_1_hidden_mul_29
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_29.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_29
  type: eltwise
  input:
  - name: round_1_hidden_sum_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_29
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_30
  type: layernorm
  input:
  - name: round_1_hidden_states_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_30.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_30.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_30.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_30
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_30
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_30
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_30.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_30.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_30
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_30
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_30
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_30.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_30.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_30
  type: concat
  input:
  - name: round_0_key_t_30
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_30
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_30
  type: concat
  input:
  - name: round_1_past_values_30
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_30
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_30
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_30
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_30
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_30.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_30
  type: softmax
  input:
  - name: round_1_qkt_mm_30
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_30
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_30
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_30
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_30
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_30
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_30.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_30
  type: linear_mv
  input:
  - name: round_1_qktv_t_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_30.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_30.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_30
  type: eltwise
  input:
  - name: round_1_hidden_states_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_30
  type: layernorm
  input:
  - name: round_1_hidden_sum_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_30.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_30.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_30
  type: silu
  input:
  - name: round_1_hidden_gate_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_30.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_30
  type: eltwise
  input:
  - name: round_1_hidden_up_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_30
  type: linear_mv
  input:
  - name: round_1_hidden_mul_30
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_30.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_30
  type: eltwise
  input:
  - name: round_1_hidden_sum_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_30
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_input_layer_norm_layer_31
  type: layernorm
  input:
  - name: round_1_hidden_states_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_states_in_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: input_layer_norm_layer_31.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_q_proj_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_before_RoPE_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: q_proj_layer_31.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: q_proj_layer_31.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_q_RoPE_eltwise_mul_sin_31
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_sin_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_mul_cos_31
  type: eltwise
  input:
  - name: round_1_q_proj_out_before_RoPE_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_RoPE_mul_cos_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_q_RoPE_eltwise_add_31
  type: eltwise
  input:
  - name: round_1_q_RoPE_mul_sin_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_q_RoPE_mul_cos_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_q_proj_out_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_k_proj_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_before_RoPE_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: k_proj_layer_31.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: true
    sparse_ratio: 0.7
  - name: k_proj_layer_31.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_k_RoPE_eltwise_mul_sin_31
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_sin_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_sin_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_mul_cos_31
  type: eltwise
  input:
  - name: round_1_k_proj_out_before_RoPE_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_RoPE_cos_cache
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_RoPE_mul_cos_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_k_RoPE_eltwise_add_31
  type: eltwise
  input:
  - name: round_1_k_RoPE_mul_sin_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_RoPE_mul_cos_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_k_proj_out_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_v_proj_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_states_in_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_v_proj_out_t_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: v_proj_layer_31.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: v_proj_layer_31.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_concat_past_keys_31
  type: concat
  input:
  - name: round_0_key_t_31
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_k_proj_out_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_key_t_31
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_concat_past_values_31
  type: concat
  input:
  - name: round_1_past_values_31
    shape:
    - 32
    - 1855
    - 128
    dtype: int8
    addr: 0
  - name: round_1_v_proj_out_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_2_past_values_31
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  structure:
    dim: 1
- name: round_1_attention_qkt_31
  type: attention_mv
  input:
  - name: round_1_q_proj_out_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  - name: round_1_key_t_31
    shape:
    - 32
    - 128
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_mm_31
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_31.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_softmax_31
  type: softmax
  input:
  - name: round_1_qkt_mm_31
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  output:
  - name: round_1_qkt_softmax_31
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  structure:
    dim: 2
- name: round_1_attention_qktv_31
  type: attention_mv
  input:
  - name: round_1_qkt_softmax_31
    shape:
    - 32
    - 1
    - 1856
    dtype: int8
    addr: 0
  - name: round_2_past_values_31
    shape:
    - 32
    - 1856
    - 128
    dtype: int8
    addr: 0
  output:
  - name: round_1_qktv_t_31
    shape:
    - 32
    - 1
    - 128
    dtype: int8
    addr: 0
  param:
  - name: attention_mask_31.mask
    shape:
    - 32
    - 64
    - 64
    dtype: bool
    addr: 0
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_out_proj_layer_31
  type: linear_mv
  input:
  - name: round_1_qktv_t_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_attn_output_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: out_proj_layer_31.weight
    shape:
    - 1
    - 4096
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  - name: out_proj_layer_31.bias
    shape:
    - 1
    - 1
    - 4096
    dtype: int32
    addr: 0
  structure:
    bias_flag: true
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_sum_layer_31
  type: eltwise
  input:
  - name: round_1_hidden_states_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_attn_output_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_sum_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_31
  type: layernorm
  input:
  - name: round_1_hidden_sum_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_layernorm_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_31.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_gate_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_gate_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: gate_layer_31.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_activation_fn_layer_31
  type: silu
  input:
  - name: round_1_hidden_gate_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_silu_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure: {}
- name: round_1_up_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_layernorm_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_up_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  param:
  - name: up_layer_31.weight
    shape:
    - 1
    - 4096
    - 11008
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_mul_layer_31
  type: eltwise
  input:
  - name: round_1_hidden_up_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  - name: round_1_hidden_silu_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_mul_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_mul
- name: round_1_down_layer_31
  type: linear_mv
  input:
  - name: round_1_hidden_mul_31
    shape:
    - 1
    - 1
    - 11008
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_down_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: down_layer_31.weight
    shape:
    - 1
    - 11008
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_hidden_add_layer_31
  type: eltwise
  input:
  - name: round_1_hidden_sum_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  - name: round_1_hidden_down_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_hidden_add_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  structure:
    eltwise_type: eltwise_add
- name: round_1_final_layer_norm_layer_in_decoder
  type: layernorm
  input:
  - name: round_1_hidden_add_31
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_final_hidden_states
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  param:
  - name: final_layer_norm_layer_in_decoder.weight
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    RMS_flag: true
- name: round_1_lm_head_layer
  type: linear_mv
  input:
  - name: round_1_final_hidden_states
    shape:
    - 1
    - 1
    - 4096
    dtype: int8
    addr: 0
  output:
  - name: round_1_logits
    shape:
    - 1
    - 1
    - 32000
    dtype: int8
    addr: 0
  param:
  - name: lm_head_layer.weight
    shape:
    - 1
    - 4096
    - 32000
    dtype: int8
    addr: 0
    sparse_flag: false
    sparse_ratio: 1
  structure:
    bias_flag: false
    mask_mode: none
    relu_flag: false
- name: round_1_output_layer
  type: output
  input:
  - name: round_1_logits
    shape:
    - 1
    - 1
    - 32000
    dtype: int8
    addr: 0
  output: []
  structure: {}
